\documentclass[11pt,a4paper]{article}

% =========================
% Packages
% =========================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
]{hyperref}
\usepackage[margin=2.2cm]{geometry}
\usepackage{float}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{parskip}
\usepackage{multirow}

% =========================
% Colors
% =========================
\definecolor{sectioncolor}{RGB}{30,30,30}
\definecolor{accent}{RGB}{0,102,204}
\definecolor{shadecolor}{RGB}{245,248,255}
\definecolor{tablegray}{gray}{0.97}

% =========================
% Page style
% =========================
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\sffamily\thepage}
\renewcommand{\headrulewidth}{0pt}

% =========================
% Section formatting
% =========================
\titleformat{\section}
    {\Large\sffamily\bfseries\color{accent}}
    {\thesection}{1em}{}

\titleformat{\subsection}
    {\large\sffamily\bfseries\color{sectioncolor}}
    {\thesubsection}{1em}{}

\titleformat{\subsubsection}
    {\normalsize\sffamily\itshape\color{sectioncolor}}
    {\thesubsubsection}{1em}{}

\titlespacing*{\section}{0pt}{2.2ex}{1.2ex}
\titlespacing*{\subsection}{0pt}{1.5ex}{1ex}
\titlespacing*{\subsubsection}{0pt}{1ex}{0.7ex}

% =========================
% Lists
% =========================
\setlist{nosep, leftmargin=1.5em}

% =========================
% Tables & floats
% =========================
\renewcommand{\arraystretch}{1.15}
\setlength{\textfloatsep}{10pt}
\setlength{\floatsep}{8pt}
\setlength{\intextsep}{8pt}

\captionsetup{
    labelfont=bf,
    font=small,
    labelsep=period,
    tableposition=top
}

% =========================
% Code listing style
% =========================
\lstset{
        basicstyle=\ttfamily\footnotesize,
        keywordstyle=\color{accent}\bfseries,
        commentstyle=\color{gray},
        stringstyle=\color{green!60!black},
        breaklines=true,
        frame=single,
        aboveskip=6pt,
        belowskip=6pt,
        backgroundcolor=\color{shadecolor},
        numbers=left,
        numberstyle=\tiny\color{gray},
        xleftmargin=2em
}

% =========================
% Custom boxes
% =========================
\tcbuselibrary{breakable}
\newtcolorbox{infobox}[1][]{
    colback=shadecolor,
    colframe=accent,
    fonttitle=\bfseries,
    title=#1,
    breakable,
    left=1mm, right=1mm, top=1mm, bottom=1mm
}

% =========================
% Title
% =========================
\title{
    \vspace{-2em}
    \color{accent}\sffamily\bfseries
    \Huge Word2Vec Implementation Report\\[0.5ex]
    \Large Skip-gram with Negative Sampling on Shakespeare Corpus
    \vspace{0.5em}
}
\author{\sffamily GenAI Course -- UM6P-CS}
\date{\sffamily\today}

\begin{document}

\maketitle

\vspace{-2em}
\begin{center}
\begin{minipage}{0.95\textwidth}
\centering
\rule{\textwidth}{0.5pt}
\end{minipage}
\end{center}

\tableofcontents

\vspace{1em}

%==============================================================================
\section{Introduction}
%==============================================================================

\begin{infobox}[Introduction]
Word embeddings are dense vector representations of words that capture semantic and syntactic relationships. The Word2Vec model, introduced by Mikolov et al. (2013), revolutionized natural language processing by providing efficient methods to learn these representations from large text corpora.

This report documents the implementation of Word2Vec using the Skip-gram architecture with negative sampling, trained on Shakespeare's complete works. We analyze:
\begin{itemize}
        \item Dataset preprocessing and characteristics
        \item Model architecture and mathematical foundations
        \item Hyperparameter selection and their effects
        \item Training dynamics and convergence behavior
        \item Quantitative and qualitative evaluation of learned embeddings
\end{itemize}
\end{infobox}

%==============================================================================
\section{Dataset}
%==============================================================================

\subsection{Data Source}

The training corpus consists of the complete works of William Shakespeare, obtained from TensorFlow's public datasets. The text file contains plays, sonnets, and poems written in Early Modern English.

\subsection{Sample Data}

\begin{tcolorbox}[colback=white, colframe=accent, title=Corpus Sample, left=1mm, right=1mm, top=0.5mm, bottom=0.5mm]
\textit{First Citizen: Before we proceed any further, hear me speak.}\\
\textit{All: Speak, speak.}\\
\textit{First Citizen: You are all resolved rather to die than to famish?}\\
\textit{All: Resolved. resolved.}\\
\textit{First Citizen: First, you know Caius Marcius is chief enemy to the people.}
\end{tcolorbox}

\subsection{Preprocessing Pipeline}

\begin{infobox}[Preprocessing Steps]
The text preprocessing involves several steps:
\begin{enumerate}
        \item \textbf{Text Loading}: Lines are loaded using TensorFlow's \texttt{TextLineDataset}, filtering empty lines.
        \item \textbf{Standardization}: A custom function converts text to lowercase and removes punctuation.
        \item \textbf{Vectorization}: The \texttt{TextVectorization} layer maps words to integer indices with:
        \begin{itemize}
                \item Vocabulary size: 4,096 tokens
                \item Sequence length: 10 tokens per line
                \item Padding applied to shorter sequences
        \end{itemize}
\end{enumerate}
\end{infobox}

\subsection{Dataset Statistics}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Dataset Characteristics}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total sequences & 32,777 \\
Vocabulary size & 4,096 \\
Sequence length & 10 \\
Training samples & $\sim$650,000 skip-gram pairs \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Model Architecture}
%==============================================================================

\subsection{Skip-gram Model}

The Skip-gram model learns word embeddings by predicting context words given a target word. For a target word $w_t$ at position $t$ in a sentence, the model predicts context words $w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}$ within a window of size $c$.

\begin{equation}
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)
\end{equation}

where the probability is defined using the softmax function:
\begin{equation}
P(w_O | w_I) = \frac{\exp(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I})}{\sum_{w=1}^{V} \exp(\mathbf{v}'_w \cdot \mathbf{v}_{w_I})}
\end{equation}

\subsection{Negative Sampling}

Computing the full softmax over the entire vocabulary is computationally expensive. Negative sampling approximates this by transforming the problem into binary classification:

\begin{equation}
\mathcal{L}_{NS} = \log \sigma(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-\mathbf{v}'_{w_i} \cdot \mathbf{v}_{w_I}) \right]
\end{equation}

where $k$ is the number of negative samples and $P_n(w)$ is the noise distribution (log-uniform in our implementation).

\subsection{Training Data Generation}

\begin{infobox}[Training Data Generation]
\begin{enumerate}
        \item Generate positive skip-gram pairs using \texttt{tf.keras.preprocessing.sequence.skipgrams}
        \item For each positive pair, sample $k$ negative context words using \texttt{tf.random.log\_uniform\_candidate\_sampler}
        \item Concatenate positive and negative samples with labels (1 for positive, 0 for negative)
\end{enumerate}
\end{infobox}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{iimage.png}
\caption{Skip-gram training data generation with negative sampling. Green boxes represent positive pairs, red boxes represent negative samples.}
\label{fig:skipgram}
\end{figure}

\subsection{Neural Network Architecture}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Model Architecture}
\begin{tabular}{llr}
\toprule
\textbf{Layer} & \textbf{Shape} & \textbf{Parameters} \\
\midrule
Target Embedding & $(4096, 256)$ & 1,048,576 \\
Context Embedding & $(4096, 256)$ & 1,048,576 \\
\midrule
\textbf{Total} & & \textbf{2,097,152} \\
\bottomrule
\end{tabular}
\end{table}

The forward pass computes:
\begin{equation}
\text{logits} = \mathbf{W}_{\text{target}}[w_t] \cdot \mathbf{W}_{\text{context}}[w_c]^T
\end{equation}
using Einstein summation notation: \texttt{tf.einsum('be,bce->bc', word\_emb, context\_emb)}.

%==============================================================================
\section{Hyperparameter Analysis}
%==============================================================================

\subsection{Chosen Hyperparameters}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Hyperparameter Configuration}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Justification} \\
\midrule
Embedding dimension & 256 & Balance between expressiveness and computational cost \\
Vocabulary size & 4,096 & Covers most frequent words in Shakespeare \\
Window size & 2 & Captures local context; larger windows capture broader semantics \\
Negative samples & 4 & Standard choice; more samples slow training \\
Batch size & 1,024 & Efficient GPU utilization \\
Learning rate & 0.01 & Relatively high for fast convergence \\
Epochs & 20 & Sufficient for convergence \\
Optimizer & Adam & Adaptive learning rates for faster convergence \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Effect of Hyperparameters}

\subsubsection{Embedding Dimension}

\begin{itemize}
        \item \textbf{Lower dimensions (50-100)}: Faster training, less expressive
        \item \textbf{Higher dimensions (300-512)}: More expressive but risk overfitting on small corpora
        \item \textbf{Chosen (256)}: Appropriate for the corpus size ($\sim$32K sequences)
\end{itemize}

\subsubsection{Window Size}

\begin{itemize}
        \item \textbf{Small windows (1-2)}: Capture syntactic relationships
        \item \textbf{Large windows (5-10)}: Capture semantic/topical relationships
        \item \textbf{Chosen (2)}: Focuses on local syntactic patterns
\end{itemize}

\subsubsection{Negative Samples}

\begin{itemize}
        \item \textbf{Fewer samples (2-5)}: Faster training, may underfit
        \item \textbf{More samples (15-20)}: Better quality for large corpora
        \item \textbf{Chosen (4)}: Standard recommendation for medium corpora
\end{itemize}

%==============================================================================
\section{Training Results}
%==============================================================================

\subsection{Training Dynamics}

The model was trained for 20 epochs. Figure~\ref{fig:training} shows the training loss and accuracy curves.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{image.png}
\caption{Training loss (left) and accuracy (right) over 20 epochs.}
\label{fig:training}
\end{figure}

\subsection{Convergence Analysis}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Training Metrics}
\begin{tabular}{lrr}
\toprule
\textbf{Epoch} & \textbf{Loss} & \textbf{Accuracy} \\
\midrule
1 & 1.52 & 0.30 \\
5 & 0.18 & 0.93 \\
10 & 0.14 & 0.95 \\
20 & 0.13 & 0.95 \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
        \item \textbf{Rapid initial convergence}: Loss drops from 1.52 to 0.18 in first 5 epochs
        \item \textbf{Plateau phase}: Minimal improvement after epoch 10
        \item \textbf{High final accuracy}: 95\% on the binary classification task (distinguishing positive from negative samples)
\end{itemize}

%==============================================================================
\section{Model Evaluation}
%==============================================================================

\subsection{Embedding Statistics}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Embedding Matrix Statistics}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Matrix shape & $(4096, 256)$ \\
Norm mean & Variable per word \\
Norm std & Variable per word \\
\bottomrule
\end{tabular}
\end{table}

Example word statistics:
\begin{itemize}
        \item \textbf{`love'}: norm = 6.41, mean = 0.0015, std = 0.40
        \item \textbf{`king'}: norm = 4.48, mean = 0.024, std = 0.28
\end{itemize}

\subsection{Word Similarity Tests}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Top 5 Similar Words}
\begin{tabular}{llr}
\toprule
\textbf{Query Word} & \textbf{Similar Word} & \textbf{Similarity} \\
\midrule
\multirow{5}{*}{king} & iii & 0.387 \\
& honour'd & 0.353 \\
& proudest & 0.344 \\
& wants & 0.339 \\
& commands & 0.336 \\
\midrule
\multirow{5}{*}{love} & innocence & 0.483 \\
& estate & 0.423 \\
& behalf & 0.420 \\
& instruments & 0.417 \\
& consorted & 0.417 \\
\midrule
\multirow{5}{*}{death} & denied & 0.375 \\
& volsce & 0.341 \\
& page & 0.329 \\
& walter & 0.327 \\
& unto & 0.326 \\
\midrule
\multirow{5}{*}{heart} & length & 0.432 \\
& infection & 0.406 \\
& weapon & 0.396 \\
& flesh & 0.392 \\
& deep & 0.381 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Analysis of Similarity Results}

\begin{itemize}
        \item \textbf{`king'}: Associates with `commands', `proudest', and Roman numerals (iii, iv) due to play act/scene numbering
        \item \textbf{`love'}: Connects to abstract concepts (`innocence', `behalf') common in romantic contexts
        \item \textbf{`heart'}: Links to body-related terms (`flesh', `weapon') and emotional depth (`deep`)
        \item Similarity scores (0.3-0.5) are moderate, typical for domain-specific corpora
\end{itemize}

\subsection{Word Analogy Tests}

\begin{table}[H]
\centering
\rowcolors{2}{tablegray}{white}
\caption{Word Analogy Results}
\begin{tabular}{llll}
\toprule
\textbf{Analogy} & \textbf{Expected} & \textbf{Predicted} & \textbf{Score} \\
\midrule
man : king :: woman : ? & queen & fame & 0.295 \\
& & saint & 0.294 \\
& & ungentle & 0.291 \\
\midrule
good : better :: bad : ? & worse & forget & 0.385 \\
& & stocks & 0.376 \\
& & withdraw & 0.367 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Discussion of Analogy Performance}

The analogy tests show limited success, attributable to:
\begin{enumerate}
        \item \textbf{Corpus specificity}: Shakespeare's language patterns differ from modern English
        \item \textbf{Small vocabulary}: 4,096 tokens may not include all needed words
        \item \textbf{Domain bias}: Literary text contains different co-occurrence patterns than general corpora
        \item \textbf{Training data size}: $\sim$32K sequences is relatively small for learning abstract relationships
\end{enumerate}

\subsection{Pairwise Similarity Distribution}

Analysis of pairwise cosine similarities across sampled embeddings reveals the overall structure of the embedding space:

\begin{itemize}
        \item \textbf{Mean similarity}: Near 0 (well-distributed embeddings)
        \item \textbf{Standard deviation}: Indicates spread of similarity values
        \item Distribution should be approximately normal for well-trained embeddings
\end{itemize}

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Strengths}

\begin{enumerate}
        \item \textbf{Efficient training}: Negative sampling enables training on modest hardware
        \item \textbf{Good convergence}: Model reaches 95\% accuracy within 10 epochs
        \item \textbf{Meaningful similarities}: Related words cluster together in embedding space
        \item \textbf{Domain adaptation}: Embeddings capture Shakespeare-specific language patterns
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
        \item \textbf{Analogy performance}: Poor on standard analogy tests due to domain specificity
        \item \textbf{Vocabulary coverage}: 4,096 tokens may miss important words
        \item \textbf{Static embeddings}: Each word has one representation regardless of context (polysemy not handled)
        \item \textbf{Window size trade-off}: Small window (2) may miss broader semantic relationships
\end{enumerate}

\subsection{Potential Improvements}

\begin{enumerate}
        \item \textbf{Increase vocabulary size}: Expand to 8,192 or 16,384 tokens
        \item \textbf{Subword embeddings}: Use BPE or character-level models for rare words
        \item \textbf{Larger corpus}: Combine with other Early Modern English texts
        \item \textbf{Hyperparameter tuning}: Grid search over embedding dimensions and window sizes
        \item \textbf{Contextual embeddings}: Upgrade to ELMo or transformer-based models
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

\begin{infobox}[Conclusion]
This report presented a Word2Vec implementation using Skip-gram with negative sampling, trained on Shakespeare's complete works. The model successfully learns meaningful word embeddings, as evidenced by:
\begin{itemize}
        \item Rapid convergence to 95\% training accuracy
        \item Semantically coherent word similarity results
        \item Captured domain-specific language patterns
\end{itemize}

The moderate performance on word analogies highlights the challenges of applying Word2Vec to specialized, historical text corpora. Future work could explore larger vocabularies, additional training data, and contextual embedding methods to improve representation quality.

The learned embeddings provide valuable semantic representations for downstream NLP tasks on Shakespearean text, including text classification, semantic search, and literary analysis.
\end{infobox}

%==============================================================================
\section*{References}
%==============================================================================

\begin{enumerate}
        \item TensorFlow. (2023). Word2Vec Tutorial. \url{https://www.tensorflow.org/tutorials/text/word2vec}
\end{enumerate}

\end{document}